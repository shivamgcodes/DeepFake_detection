{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_gqBavH_Nf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = InceptionResnetV1(\n",
        "    pretrained=\"vggface2\",\n",
        "    classify=True,\n",
        "    num_classes=1,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy for single-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "!pip install facenet-pytorch\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
        "!pip install facenet_pytorch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import tqdm as tqdm\n",
        "#l1_strength = 0.001  # Adjust this parameter based on your preference\n",
        "# Optimizer with L1 Regularization\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=l1_strength)\n",
        "\n",
        "mtcnn = MTCNN(\n",
        "    select_largest=False,\n",
        "    post_process=False,\n",
        "    device=DEVICE\n",
        ").to(DEVICE).eval()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dir = \"/kaggle/input/deepfake-and-real-images/Dataset/Train/\"\n",
        "\n",
        "training_data = CustomImageDataset(\"/kaggle/input/deepfake-and-real-images/Dataset/Validation/Real\", \"/kaggle/input/deepfake-and-real-images/Dataset/Validation/Fake\" , transform)\n",
        "\n",
        "\n",
        "print(len(training_data))\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "num_epochs = 30"
      ],
      "metadata": {
        "id": "_hqAjqDC_C7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpzInqtn-4eU"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(299),  # InceptionResNetV1 input size\n",
        "    transforms.CenterCrop(299),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self,  real_dir , fake_dir, transform=None, target_transform=None):\n",
        "        #self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.real_dir = real_dir\n",
        "        self.fake_dir = fake_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.real_list = os.listdir(self.real_dir)\n",
        "        self.fake_list = os.listdir(self.fake_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(os.listdir(self.real_dir)) + len(os.listdir(self.fake_dir)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = 0\n",
        "        if(idx > len(self.real_list)):\n",
        "            idx = idx - len(self.real_list)\n",
        "            file = self.fake_list[idx]\n",
        "            image = Image.open(os.path.join(self.fake_dir, file))\n",
        "            label = 0\n",
        "\n",
        "        else:\n",
        "           file = self.real_list[idx]\n",
        "           image = Image.open(os.path.join(self.real_dir, file))\n",
        "           label = 1\n",
        "        if self.transform is not None:\n",
        "\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    torch.save(model, 'epoch' + str(epoch) + 'deepfake_detection.pt')\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "\n",
        "    for inputs, labels in train_dataloader:\n",
        "      model.train()\n",
        "      inputs = torch.tensor(inputs)\n",
        "      labels = torch.tensor(labels)\n",
        "      inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs.view(-1), labels.float())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataloader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "id": "ChLc-8yq-8yQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}